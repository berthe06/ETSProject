{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0B9DL5gXY22G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "# -------------------- MAPPING DES TEMPLATES -------------------- #\n",
        "def mapping(file_names, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for file_name in file_names:\n",
        "        log_templates_file = os.path.join(output_dir, file_name)\n",
        "        log_temp = pd.read_csv(log_templates_file).sort_values(by=\"Occurrences\", ascending=False)\n",
        "        log_temp_dict = {event: f\"E{idx + 1}\" for idx, event in enumerate(log_temp[\"EventId\"])}\n",
        "        output_file = os.path.join(output_dir, f\"{file_name.replace('.csv', '')}.json\")\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(log_temp_dict, f)\n",
        "        print(f\"Mapping saved to {output_file}\")\n",
        "\n",
        "# -------------------- PRÉTRAITEMENT DES LOGS STRUCTURÉS -------------------- #\n",
        "def process_log_files(input_dir, output_dir, json_filename, structured_log_filename, anomaly_label_filename, output_filename):\n",
        "    json_file_path = os.path.join(output_dir, json_filename)\n",
        "    anomaly_label_path = os.path.join(input_dir, anomaly_label_filename)\n",
        "    structured_log_path = os.path.join(output_dir, structured_log_filename)\n",
        "\n",
        "    df_structured = pd.read_csv(structured_log_path)\n",
        "    with open(json_file_path, 'r') as json_file:\n",
        "        event_mapping = json.load(json_file)\n",
        "    df_labels = pd.read_csv(anomaly_label_path)\n",
        "    df_labels['Label'] = df_labels['Label'].replace({'Normal': 'Success', 'Anomaly': 'Fail'})\n",
        "\n",
        "    df_structured['BlockId'] = df_structured['Content'].apply(lambda x: re.search(r'blk_(|-)[0-9]+', x).group(0) if re.search(r'blk_(|-)[0-9]+', x) else None)\n",
        "    df_structured = df_structured.dropna(subset=['BlockId'])\n",
        "    df_structured['EventId'] = df_structured['EventId'].apply(lambda x: event_mapping.get(x, x))\n",
        "    df_structured = pd.merge(df_structured, df_labels, on='BlockId', how='left')\n",
        "\n",
        "    columns = ['BlockId', 'Label'] + [col for col in df_structured.columns if col not in ['BlockId', 'Label']]\n",
        "    df_structured = df_structured[columns]\n",
        "\n",
        "    output_path = os.path.join(output_filename)\n",
        "    df_structured.to_csv(output_path, index=False)\n",
        "    print(f\"Processed log saved to {output_path}\")\n",
        "\n",
        "# -------------------- SAMPLING PAR SESSION -------------------- #\n",
        "def hdfs_sampling(file_names, input_dir, output_dir, window='session', window_size=0):\n",
        "    assert window == 'session', \"Only window=session is supported.\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in file_names:\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "        output_path = os.path.join(output_dir, file_name.replace('.csv', '_sequence.csv'))\n",
        "\n",
        "        struct_log = pd.read_csv(input_path, engine='c', na_filter=False, memory_map=True, dtype={'Time': str})\n",
        "        struct_log['Time'] = struct_log['Time'].str.zfill(6)\n",
        "        struct_log['Date'] = struct_log['Date'].astype(str).str.zfill(6)\n",
        "        struct_log['BlockId'] = struct_log['Content'].str.extract(r'(blk_-?\\d+)')\n",
        "        struct_log['EventId'] = struct_log['EventId'].fillna('')\n",
        "        struct_log['Label'] = struct_log['Label'].apply(lambda x: 1 if x == 'Fail' else 0)\n",
        "\n",
        "        data_dict, time_dict, date_dict, type_count = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(int)\n",
        "\n",
        "        grouped = struct_log.groupby('BlockId')\n",
        "        for block_id, group in tqdm(grouped, total=len(grouped)):\n",
        "            data_dict[block_id] = group['EventId'].tolist()\n",
        "            time_dict[block_id] = pd.to_datetime(group['Time'], format='%H%M%S', errors='coerce').dropna()\n",
        "            date_dict[block_id] = group['Date'].tolist()\n",
        "            type_count[block_id] = group['Label'].sum()\n",
        "\n",
        "        rows = []\n",
        "        for block_id, events in tqdm(data_dict.items(), total=len(data_dict)):\n",
        "            features = [event for event in events if event]\n",
        "            times = time_dict[block_id]\n",
        "            dates = date_dict[block_id]\n",
        "            time_intervals = [(times.iloc[i] - times.iloc[i - 1]).total_seconds() for i in range(1, len(times))] if len(times) > 1 else []\n",
        "            latency = (times.iloc[-1] - times.iloc[0]).total_seconds() if len(times) > 1 else 0\n",
        "            label = 'Fail' if type_count[block_id] > 0 else 'Success'\n",
        "            first_date = dates[0] if dates else ''\n",
        "            first_time = times.iloc[0].strftime('%H%M%S') if not times.empty else ''\n",
        "\n",
        "            rows.append({\n",
        "                \"BlockId\": block_id,\n",
        "                \"Label\": label,\n",
        "                \"Type\": type_count[block_id],\n",
        "                \"Features\": str(features),\n",
        "                \"Date\": first_date,\n",
        "                \"Time\": first_time,\n",
        "                \"TimeInterval\": str(time_intervals),\n",
        "                \"Latency\": latency\n",
        "            })\n",
        "\n",
        "        data_df = pd.DataFrame(rows, columns=['BlockId', 'Label', 'Type', 'Features', 'Date', 'Time', 'TimeInterval', 'Latency'])\n",
        "        data_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_train.log_structured.csv'\n",
        "print(\"✅ Existe :\", os.path.exists(path))\n",
        "\n",
        "\n",
        "target_dir = \"/content/drive/MyDrive/ProjetEts/HDFS_results/\"\n",
        "\n",
        "for f in os.listdir(target_dir):\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4l0lewn7SVB",
        "outputId": "acbb8d3e-7188-4e6a-bae0-a73af56aab37"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Existe : True\n",
            "Data_Preprocessing.py\n",
            "Event_occurence_matrix_HDFS_train.csv\n",
            "Event_occurence_matrix_HDFS_valid.csv\n",
            "Event_occurence_matrix_HDFS_test.csv\n",
            "Event_occurence_matrix_HDFS_test_predicted.csv\n",
            "HDFS_train.log_structured_blk_sequence.csv\n",
            "HDFS_valid.log_structured_blk_sequence.csv\n",
            "HDFS_test.log_structured_blk_sequence.csv\n",
            "Event_traces.csv\n",
            "Event_occurrence_matrix.csv\n",
            "anomaly_label.csv\n",
            "HDFS_train.log_structured.csv\n",
            "HDFS_train.log_templates.csv\n",
            "HDFS_valid.log_structured.csv\n",
            "HDFS_valid.log_templates.csv\n",
            "HDFS_test.log_structured.csv\n",
            "HDFS_test.log_templates.csv\n",
            "HDFS_train.log_structured_blk.csv\n",
            "HDFS_valid.log_structured_blk.csv\n",
            "HDFS_test.log_structured_blk.csv\n",
            "HDFS_train.log_templates.json\n",
            "HDFS_test.log_templates.json\n",
            "HDFS_valid.log_templates.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========================\n",
        "# Fonctions Utilitaires\n",
        "# ========================\n",
        "\n",
        "def mount_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"Drive mounted\")\n",
        "\n",
        "print(os.path.exists('/content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_train.log_structured.csv'))\n",
        "\n",
        "\n",
        "def process_log_files(input_dir, output_dir, json_filename, structured_log_filename, anomaly_label_filename, output_filename):\n",
        "    json_file_path = os.path.join(output_dir, json_filename)\n",
        "    anomaly_label_path = os.path.join(input_dir, anomaly_label_filename)\n",
        "    structured_log_path = os.path.join(output_dir, structured_log_filename)\n",
        "\n",
        "    df_structured = pd.read_csv(structured_log_path)\n",
        "    with open(json_file_path, 'r') as json_file:\n",
        "        event_mapping = json.load(json_file)\n",
        "    df_labels = pd.read_csv(anomaly_label_path)\n",
        "    df_labels['Label'] = df_labels['Label'].replace({'Normal': 'Success', 'Anomaly': 'Fail'})\n",
        "\n",
        "    df_structured['BlockId'] = df_structured['Content'].apply(lambda x: re.search(r'blk_(|-)[0-9]+', x).group(0) if re.search(r'blk_(|-)[0-9]+', x) else None)\n",
        "    df_structured = df_structured.dropna(subset=['BlockId'])\n",
        "    df_structured['EventId'] = df_structured['EventId'].apply(lambda x: event_mapping.get(x, x))\n",
        "    df_structured = pd.merge(df_structured, df_labels, on='BlockId', how='left')\n",
        "\n",
        "    columns = ['BlockId', 'Label'] + [col for col in df_structured.columns if col not in ['BlockId', 'Label']]\n",
        "    df_structured = df_structured[columns]\n",
        "\n",
        "    df_structured.to_csv(output_filename, index=False)\n",
        "    print(f\"✅ Fichier généré : {output_filename}\")\n",
        "\n",
        "def hdfs_sampling(file_names, input_dir, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for file_name in file_names:\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "        output_path = os.path.join(output_dir, file_name.replace('.csv', '_sequence.csv'))\n",
        "\n",
        "        struct_log = pd.read_csv(input_path, engine='c', na_filter=False, memory_map=True, dtype={'Time': str})\n",
        "        struct_log['Time'] = struct_log['Time'].str.zfill(6)\n",
        "        struct_log['Date'] = struct_log['Date'].astype(str).str.zfill(6)\n",
        "        struct_log['BlockId'] = struct_log['Content'].str.extract(r'(blk_-?\\d+)')\n",
        "        struct_log['EventId'] = struct_log['EventId'].fillna('')\n",
        "        struct_log['Label'] = struct_log['Label'].apply(lambda x: 1 if x == 'Fail' else 0)\n",
        "\n",
        "        data_dict = defaultdict(list)\n",
        "        time_dict = defaultdict(list)\n",
        "        date_dict = defaultdict(list)\n",
        "        type_count = defaultdict(int)\n",
        "\n",
        "        grouped = struct_log.groupby('BlockId')\n",
        "        for block_id, group in tqdm(grouped, total=len(grouped)):\n",
        "            data_dict[block_id] = group['EventId'].tolist()\n",
        "            time_dict[block_id] = pd.to_datetime(group['Time'], format='%H%M%S', errors='coerce').dropna()\n",
        "            date_dict[block_id] = group['Date'].tolist()\n",
        "            type_count[block_id] = group['Label'].sum()\n",
        "\n",
        "        rows = []\n",
        "        for block_id, events in tqdm(data_dict.items(), total=len(data_dict)):\n",
        "            features = [event for event in events if event]\n",
        "            times = time_dict[block_id]\n",
        "            dates = date_dict[block_id]\n",
        "            if len(times) > 1:\n",
        "                time_intervals = [(times.iloc[i] - times.iloc[i - 1]).total_seconds() for i in range(1, len(times))]\n",
        "                latency = (times.iloc[-1] - times.iloc[0]).total_seconds()\n",
        "            else:\n",
        "                time_intervals = []\n",
        "                latency = 0\n",
        "            label = 'Fail' if type_count[block_id] > 0 else 'Success'\n",
        "            first_date = dates[0] if dates else ''\n",
        "            first_time = times.iloc[0].strftime('%H%M%S') if not times.empty else ''\n",
        "            rows.append({\n",
        "                \"BlockId\": block_id,\n",
        "                \"Label\": label,\n",
        "                \"Type\": type_count[block_id],\n",
        "                \"Features\": str(features),\n",
        "                \"Date\": first_date,\n",
        "                \"Time\": first_time,\n",
        "                \"TimeInterval\": str(time_intervals),\n",
        "                \"Latency\": latency\n",
        "            })\n",
        "\n",
        "        data_df = pd.DataFrame(rows)\n",
        "        data_df.to_csv(output_path, index=False)\n",
        "        print(f\"✅ HDFS sampling terminé : {output_path}\")\n",
        "\n",
        "def generate_event_occurrence_matrix(log_files, event_traces_files, input_dir, output_dir, event_columns=None):\n",
        "    if event_columns is None:\n",
        "        event_columns = [f\"E{i}\" for i in range(1, 30)]\n",
        "    anomaly_label_file = os.path.join(input_dir, \"anomaly_label.csv\")\n",
        "    anomaly_labels = pd.read_csv(anomaly_label_file)\n",
        "    anomaly_labels['Label'] = anomaly_labels['Label'].apply(lambda x: 'Fail' if x == 'Anomaly' else 'Success')\n",
        "    label_dict = anomaly_labels.set_index('BlockId')['Label'].to_dict()\n",
        "\n",
        "    for log_file, event_traces_file in zip(log_files, event_traces_files):\n",
        "        output_file = os.path.join(output_dir, f\"Event_occurence_matrix_{log_file.replace('.log', '')}.csv\")\n",
        "        print(f\"Processing {log_file}...\")\n",
        "        event_traces = pd.read_csv(event_traces_file)\n",
        "        occurrence_matrix = []\n",
        "\n",
        "        for _, row in event_traces.iterrows():\n",
        "            block_id = row['BlockId']\n",
        "            label = label_dict.get(block_id, 'Unknown')\n",
        "            event_list = re.findall(r\"E\\d+\", row['Features'])\n",
        "            event_counts = {event: event_list.count(event) for event in event_columns}\n",
        "            occurrence_matrix.append({\n",
        "                \"BlockId\": block_id,\n",
        "                \"Label\": label,\n",
        "                \"Type\": int(row['Type']) if pd.notna(row['Type']) else 0,\n",
        "                \"Time\": row.get('Time', ''),\n",
        "                \"Date\": row.get('Date', ''),\n",
        "                **event_counts\n",
        "            })\n",
        "\n",
        "        occurrence_matrix_df = pd.DataFrame(occurrence_matrix)\n",
        "        occurrence_matrix_df.to_csv(output_file, index=False)\n",
        "        print(f\"✅ Matrice d'occurrence sauvegardée : {output_file}\")\n",
        "\n",
        "# ========================\n",
        "# Execution du pipeline\n",
        "# ========================\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/ProjetEts/HDFS_results/'\n",
        "output_dir = '/content/drive/MyDrive/ProjetEts/HDFS_results/'\n",
        "\n",
        "process_log_files(input_dir, output_dir, 'HDFS_train.log_templates.json', 'HDFS_train.log_structured.csv', 'anomaly_label.csv', os.path.join(output_dir, 'HDFS_train.log_structured_blk.csv'))\n",
        "process_log_files(input_dir, output_dir, 'HDFS_valid.log_templates.json', 'HDFS_valid.log_structured.csv', 'anomaly_label.csv', os.path.join(output_dir, 'HDFS_valid.log_structured_blk.csv'))\n",
        "process_log_files(input_dir, output_dir, 'HDFS_test.log_templates.json',  'HDFS_test.log_structured.csv',  'anomaly_label.csv', os.path.join(output_dir, 'HDFS_test.log_structured_blk.csv'))\n",
        "\n",
        "hdfs_sampling([\n",
        "    'HDFS_train.log_structured_blk.csv',\n",
        "    'HDFS_valid.log_structured_blk.csv',\n",
        "    'HDFS_test.log_structured_blk.csv'\n",
        "], output_dir, output_dir)\n",
        "\n",
        "generate_event_occurrence_matrix(\n",
        "    ['HDFS_train.log', 'HDFS_valid.log', 'HDFS_test.log'],\n",
        "    [\n",
        "        os.path.join(output_dir, 'HDFS_train.log_structured_blk_sequence.csv'),\n",
        "        os.path.join(output_dir, 'HDFS_valid.log_structured_blk_sequence.csv'),\n",
        "        os.path.join(output_dir, 'HDFS_test.log_structured_blk_sequence.csv')\n",
        "    ],\n",
        "    '/content/drive/MyDrive/ProjetEts/',\n",
        "    output_dir\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfK1__mYalD",
        "outputId": "f6144b06-a77b-4865-89d3-095abb51d982"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "✅ Fichier généré : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_train.log_structured_blk.csv\n",
            "✅ Fichier généré : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_valid.log_structured_blk.csv\n",
            "✅ Fichier généré : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_test.log_structured_blk.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 333405/333405 [04:33<00:00, 1220.31it/s]\n",
            "100%|██████████| 333405/333405 [03:00<00:00, 1842.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ HDFS sampling terminé : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_train.log_structured_blk_sequence.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 191205/191205 [02:30<00:00, 1274.42it/s]\n",
            "100%|██████████| 191205/191205 [01:03<00:00, 3018.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ HDFS sampling terminé : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_valid.log_structured_blk_sequence.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180558/180558 [02:30<00:00, 1202.86it/s]\n",
            "100%|██████████| 180558/180558 [01:00<00:00, 2965.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ HDFS sampling terminé : /content/drive/MyDrive/ProjetEts/HDFS_results/HDFS_test.log_structured_blk_sequence.csv\n",
            "Processing HDFS_train.log...\n",
            "✅ Matrice d'occurrence sauvegardée : /content/drive/MyDrive/ProjetEts/HDFS_results/Event_occurence_matrix_HDFS_train.csv\n",
            "Processing HDFS_valid.log...\n",
            "✅ Matrice d'occurrence sauvegardée : /content/drive/MyDrive/ProjetEts/HDFS_results/Event_occurence_matrix_HDFS_valid.csv\n",
            "Processing HDFS_test.log...\n",
            "✅ Matrice d'occurrence sauvegardée : /content/drive/MyDrive/ProjetEts/HDFS_results/Event_occurence_matrix_HDFS_test.csv\n"
          ]
        }
      ]
    }
  ]
}